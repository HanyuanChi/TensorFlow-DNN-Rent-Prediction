{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanyuan chi\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# read data\n",
    "rent = pd.read_csv('fang88_rent.csv',header = 0, na_values=[\"NA\",\"NaN\", \" \",\"\",\"NULL\",\"N/A\",\"None\"])\n",
    "house = pd.read_csv('home.csv',header = 0, na_values=[\"NA\",\"NaN\", \" \",\"\", \"NULL\",\"N/A\",\"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       rentzestimate_amount\n",
      "count         960854.000000\n",
      "mean            2994.374168\n",
      "std             4020.805179\n",
      "min              400.000000\n",
      "25%             1550.000000\n",
      "50%             2200.000000\n",
      "75%             3200.000000\n",
      "max           250000.000000\n",
      "rentzestimate_amount    float64\n",
      "unique_id                object\n",
      "dtype: object\n",
      "(1048575, 2)\n"
     ]
    }
   ],
   "source": [
    "#pick the columns to use in rent(unique_id and rentzestimate_amount) and check the statistics of rent dataset\n",
    "rent_use = rent.iloc[:,4:6] #rent_use = rent[[\"rentzestimate_amount\",\"unique_id\"]]\n",
    "print(rent_use.describe())\n",
    "print(rent_use.dtypes)\n",
    "print(rent_use.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rentzestimate_amount    unique_id\n",
      "1902                  1800.0  5.74132E+13\n",
      "8565                     NaN  5.81157E+13\n",
      "9169                     NaN  5.82614E+13\n",
      "422191                2962.0  5.89261E+13\n"
     ]
    }
   ],
   "source": [
    "#remove duplicated unique_id in rent\n",
    "print(rent_use[rent_use.duplicated(subset = 'unique_id',keep = 'first') == True])\n",
    "rent_use = rent_use.drop_duplicates(subset = 'unique_id',keep = 'first') #4 duplicates removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           bathrooms       bedrooms    list_price       latitude  \\\n",
      "count  349570.000000  338944.000000  3.522200e+05  338114.000000   \n",
      "mean        2.478044       3.364178  6.903779e+05      33.730169   \n",
      "std         1.339578       1.313034  1.763565e+06       5.998713   \n",
      "min         0.000000       0.000000  0.000000e+00     -81.265730   \n",
      "25%         2.000000       3.000000  2.199990e+05      29.495881   \n",
      "50%         2.000000       3.000000  3.699000e+05      33.208160   \n",
      "75%         3.000000       4.000000  6.499000e+05      37.939740   \n",
      "max        99.990000      99.000000  2.500000e+08      49.002033   \n",
      "\n",
      "           longitude      lot_sqft          sqft    year_built  \n",
      "count  338114.000000  2.141130e+05  3.268170e+05  3.360800e+05  \n",
      "mean      -93.934758  1.301220e+05  2.368388e+03  2.047163e+03  \n",
      "std        16.612686  8.698167e+06  6.622419e+03  3.375856e+04  \n",
      "min      -159.676034  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%       -97.995093  5.600000e+03  1.344000e+03  1.970000e+03  \n",
      "50%       -88.116701  8.712000e+03  2.012000e+03  1.993000e+03  \n",
      "75%       -81.483450  1.864400e+04  2.961000e+03  2.006000e+03  \n",
      "max         1.923200  2.147484e+09  3.092760e+06  1.957194e+07  \n",
      "unique_id          object\n",
      " bathrooms        float64\n",
      " bedrooms         float64\n",
      " city              object\n",
      " list_price       float64\n",
      " latitude         float64\n",
      " longitude        float64\n",
      " property_type     object\n",
      " lot_sqft         float64\n",
      " sqft             float64\n",
      " state             object\n",
      " year_built       float64\n",
      " zip               object\n",
      "dtype: object\n",
      "Empty DataFrame\n",
      "Columns: [unique_id,  bathrooms,  bedrooms,  city,  list_price,  latitude,  longitude,  property_type,  lot_sqft,  sqft,  state,  year_built,  zip]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#check the statistics of house dataset and remove duplicated unique_id in house\n",
    "print(house.describe())\n",
    "print(house.dtypes)\n",
    "print(house[house.duplicated(subset = 'unique_id',keep = 'first') == True])#no duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           bathrooms       bedrooms    list_price       latitude  \\\n",
      "count  349570.000000  338944.000000  3.522200e+05  338114.000000   \n",
      "mean        2.478044       3.364178  6.903779e+05      33.730169   \n",
      "std         1.339578       1.313034  1.763565e+06       5.998713   \n",
      "min         0.000000       0.000000  0.000000e+00     -81.265730   \n",
      "25%         2.000000       3.000000  2.199990e+05      29.495881   \n",
      "50%         2.000000       3.000000  3.699000e+05      33.208160   \n",
      "75%         3.000000       4.000000  6.499000e+05      37.939740   \n",
      "max        99.990000      99.000000  2.500000e+08      49.002033   \n",
      "\n",
      "           longitude      lot_sqft          sqft    year_built  \\\n",
      "count  338114.000000  2.141130e+05  3.268170e+05  3.360800e+05   \n",
      "mean      -93.934758  1.301220e+05  2.368388e+03  2.047163e+03   \n",
      "std        16.612686  8.698167e+06  6.622419e+03  3.375856e+04   \n",
      "min      -159.676034  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%       -97.995093  5.600000e+03  1.344000e+03  1.970000e+03   \n",
      "50%       -88.116701  8.712000e+03  2.012000e+03  1.993000e+03   \n",
      "75%       -81.483450  1.864400e+04  2.961000e+03  2.006000e+03   \n",
      "max         1.923200  2.147484e+09  3.092760e+06  1.957194e+07   \n",
      "\n",
      "       rentzestimate_amount  \n",
      "count         187277.000000  \n",
      "mean            3854.405378  \n",
      "std             6320.721666  \n",
      "min              400.000000  \n",
      "25%             1600.000000  \n",
      "50%             2300.000000  \n",
      "75%             3500.000000  \n",
      "max           250000.000000  \n",
      "unique_id                object\n",
      " bathrooms              float64\n",
      " bedrooms               float64\n",
      " city                    object\n",
      " list_price             float64\n",
      " latitude               float64\n",
      " longitude              float64\n",
      " property_type           object\n",
      " lot_sqft               float64\n",
      " sqft                   float64\n",
      " state                   object\n",
      " year_built             float64\n",
      " zip                     object\n",
      "rentzestimate_amount    float64\n",
      "dtype: object\n",
      "(352221, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>city</th>\n",
       "      <th>list_price</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>property_type</th>\n",
       "      <th>lot_sqft</th>\n",
       "      <th>sqft</th>\n",
       "      <th>state</th>\n",
       "      <th>year_built</th>\n",
       "      <th>zip</th>\n",
       "      <th>rentzestimate_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABOR_15646314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>30.388230</td>\n",
       "      <td>-97.965567</td>\n",
       "      <td>RESI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>78734</td>\n",
       "      <td>1350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABOR_16633908</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Burnet</td>\n",
       "      <td>6850000.0</td>\n",
       "      <td>30.674729</td>\n",
       "      <td>-98.212187</td>\n",
       "      <td>RESI</td>\n",
       "      <td>640.0</td>\n",
       "      <td>4889.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>78611</td>\n",
       "      <td>9320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABOR_17615028</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>San Marcos</td>\n",
       "      <td>334900.0</td>\n",
       "      <td>29.866884</td>\n",
       "      <td>-97.968936</td>\n",
       "      <td>COND</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>78666</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABOR_18247858</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>735000.0</td>\n",
       "      <td>30.265435</td>\n",
       "      <td>-97.737834</td>\n",
       "      <td>APT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>78701</td>\n",
       "      <td>3569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABOR_18359329</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>2800000.0</td>\n",
       "      <td>30.325342</td>\n",
       "      <td>-97.779271</td>\n",
       "      <td>RESI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>78746</td>\n",
       "      <td>13766.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id   bathrooms   bedrooms        city   list_price   latitude  \\\n",
       "0  ABOR_15646314         1.0        1.0      Austin     225000.0  30.388230   \n",
       "1  ABOR_16633908         5.0        3.0      Burnet    6850000.0  30.674729   \n",
       "2  ABOR_17615028         3.0        3.0  San Marcos     334900.0  29.866884   \n",
       "3  ABOR_18247858         2.0        2.0      Austin     735000.0  30.265435   \n",
       "4  ABOR_18359329         3.0        4.0      Austin    2800000.0  30.325342   \n",
       "\n",
       "    longitude  property_type   lot_sqft    sqft  state   year_built    zip  \\\n",
       "0  -97.965567           RESI        0.0   704.0     TX       1955.0  78734   \n",
       "1  -98.212187           RESI      640.0  4889.0     TX       2000.0  78611   \n",
       "2  -97.968936           COND        0.0  2351.0     TX       2016.0  78666   \n",
       "3  -97.737834            APT        0.0  1416.0     TX       2004.0  78701   \n",
       "4  -97.779271           RESI        1.0  2016.0     TX       1962.0  78746   \n",
       "\n",
       "   rentzestimate_amount  \n",
       "0                1350.0  \n",
       "1                9320.0  \n",
       "2                   NaN  \n",
       "3                3569.0  \n",
       "4               13766.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#left join house with rent_use by unique_id\n",
    "house_rent = house.merge(rent_use,left_on = 'unique_id',right_on = 'unique_id',how = 'left')\n",
    "print(house_rent.describe())\n",
    "print(house_rent.dtypes)\n",
    "print(house_rent.shape)\n",
    "house_rent.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244862\n"
     ]
    }
   ],
   "source": [
    "print(len(house_rent[house_rent.isnull().any(axis=1)]))#check the number of NAs\n",
    "#fill NA with median for numerical columns expect for zip and rentzestimate_amount\n",
    "house_rent.iloc[:,:12] = house_rent.iloc[:,:12].fillna(house_rent.iloc[:,:12].median()) \n",
    "# #rename the 14th column as rentzestimate_amount\n",
    "# house_rent.columns.values[13]= 'rentzestimate_amount'\n",
    "house_rent.columns = house_rent.columns.str.strip() #strip white space from the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 19.0 99.99\n",
      "0.0 1.0 20.0 99.0\n",
      "0.0 1450.0 55000000.0 250000000.0\n",
      "0.0 0.0 6992861.04 2147483647.0\n",
      "0.0 0.0 20687.21 3092760.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanyuan chi\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#Deal with outliers and abnormal values in each column\n",
    "#bathrooms: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.bathrooms,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.bathrooms > q99_99,'bathrooms'] = q99_99\n",
    "np.percentile(house_rent.bathrooms,[0,1,99.99,100])\n",
    "\n",
    "#bedrooms: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.bedrooms,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.bedrooms > q99_99,'bedrooms'] = q99_99\n",
    "np.percentile(house_rent.bedrooms,[0,1,99.99,100])\n",
    "\n",
    "#list_price: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.list_price,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.list_price > q99_99,'list_price'] = q99_99\n",
    "np.percentile(house_rent.list_price,[0,1,99.99,100])\n",
    "\n",
    "#lot_sqft: capping of outliers--0.9995\n",
    "q0,q1,q99_95,q100 = np.percentile(house_rent.lot_sqft,[0,1,99.95,100])\n",
    "print(q0,q1,q99_95,q100)\n",
    "house_rent.loc[house_rent.lot_sqft > q99_95,'lot_sqft'] = q99_95\n",
    "np.percentile(house_rent.lot_sqft,[0,1,99.95,100])\n",
    "\n",
    "#sqft: capping of outliers--0.9995\n",
    "q0,q1,q99_95,q100 = np.percentile(house_rent.sqft,[0,1,99.95,100])\n",
    "print(q0,q1,q99_95,q100)\n",
    "house_rent.loc[house_rent.sqft > q99_95,'sqft'] = q99_95\n",
    "np.percentile(house_rent.sqft,[0,1,99.95,100])\n",
    "\n",
    "#for year_built, earlier than 1000 and later than 2017 will be replaced by median\n",
    "house_rent.loc[(house_rent.year_built < float(1000)) | \n",
    "               (house_rent.year_built > float(2017)),'year_built'] = house_rent.year_built.median()\n",
    "\n",
    "#replace hi with HI in state, Hi with HI, Ha with HA and Unk with UNK\n",
    "house_rent.loc[house_rent.state == 'hi','state'] = \"HI\"\n",
    "house_rent.loc[house_rent.state == 'Hi','state'] = \"HI\"\n",
    "house_rent.loc[house_rent.state == 'Ha','state'] = \"HA\"\n",
    "house_rent.loc[house_rent.state == 'Unk','state'] = \"UNK\"\n",
    "\n",
    "#select main states\n",
    "main_state = [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\n",
    "                                              \"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\n",
    "                                              \"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\n",
    "                                              \"TN\",\"TX\",\"UT\",\"VA\", \"VT\",\"WA\", \"WI\", \"WV\", \"WY\",\n",
    "                                              \"XX\",\"HI\",\"UNK\",\"HA\",\"BJ\"]\n",
    "house_rent = house_rent.loc[house_rent['state'].isin(main_state)]\n",
    "\n",
    "#choose rows with zip not being NA and has exactly 5 digits\n",
    "house_rent = house_rent.loc[house_rent['zip'].isnull()== False]\n",
    "house_rent.zip = house_rent.zip.astype(str).str.extract('(\\d{5})').astype(float)\n",
    "house_rent = house_rent.loc[house_rent['zip'].isnull()== False]\n",
    "#np.sort(house_rent.zip.unique())\n",
    "\n",
    "#chose rows with property_type not being NA and replace Other with OTHER\n",
    "house_rent = house_rent.loc[house_rent['property_type'].isnull()== False]\n",
    "house_rent.loc[house_rent.property_type == 'Other','property_type'] = 'OTHER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329503, 14)\n",
      "unique_id               329503\n",
      "bathrooms               329503\n",
      "bedrooms                329503\n",
      "city                    329238\n",
      "list_price              329503\n",
      "latitude                329503\n",
      "longitude               329503\n",
      "property_type           329503\n",
      "lot_sqft                329503\n",
      "sqft                    329503\n",
      "state                   329503\n",
      "year_built              329503\n",
      "zip                     329503\n",
      "rentzestimate_amount    176420\n",
      "dtype: int64\n",
      "unique_id                object\n",
      "bathrooms               float64\n",
      "bedrooms                float64\n",
      "city                     object\n",
      "list_price              float64\n",
      "latitude                float64\n",
      "longitude               float64\n",
      "property_type            object\n",
      "lot_sqft                float64\n",
      "sqft                    float64\n",
      "state                    object\n",
      "year_built              float64\n",
      "zip                     float64\n",
      "rentzestimate_amount    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check data structure\n",
    "print(house_rent.shape)\n",
    "print(house_rent.count())\n",
    "print(house_rent.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change zip code data type from float to string\n",
    "house_rent.zip = house_rent.zip.astype(str)\n",
    "#create new variable: year_from_now\n",
    "import datetime\n",
    "house_rent['year_from_now'] = datetime.date.today().year - house_rent['year_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables: create dummies--property type\n",
    "#zip,city,state have too many levels, so I don't include them in the prediction\n",
    "state = pd.get_dummies(house_rent['state']).add_suffix('_state')\n",
    "property_type = pd.get_dummies(house_rent['property_type']).add_suffix('_type')\n",
    "house_rent_dummy = pd.concat([house_rent,property_type,state],axis = 1)#combine dummy and the original dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select useful attributes\n",
    "property_price_all = house_rent_dummy.loc[:,~house_rent_dummy.columns.isin(['Unnamed: 0','unique_id','city','property_type',\n",
    "                                                                        'state','year_built','zip'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalization: (x-mean)/std (dummy variable and y(rentzestimate_amount) don't need normalization)\n",
    "property_price_norm = property_price_all\n",
    "property_price_norm = property_price_norm[['bathrooms', 'bedrooms', 'list_price', 'latitude', 'longitude',\n",
    "       'lot_sqft', 'sqft','year_from_now', 'APT_type',\n",
    "       'COND_type', 'COOP_type', 'Country Homes/Acreag_type', 'FARM_type',\n",
    "       'LAND_type', 'MULT_type', 'OTHER_type', 'RENT_type', 'RESI_type',\n",
    "       'Timeshare_type', 'AK_state', 'AL_state', 'AR_state', 'AZ_state',\n",
    "       'BJ_state', 'CA_state', 'CO_state', 'DE_state', 'FL_state', 'GA_state',\n",
    "       'HA_state', 'HI_state', 'IA_state', 'ID_state', 'IL_state', 'IN_state',\n",
    "       'KS_state', 'KY_state', 'LA_state', 'MA_state', 'MD_state', 'MI_state',\n",
    "       'MN_state', 'MO_state', 'MT_state', 'NC_state', 'NE_state', 'NH_state',\n",
    "       'NJ_state', 'NM_state', 'NV_state', 'NY_state', 'OH_state', 'OK_state',\n",
    "       'OR_state', 'PA_state', 'RI_state', 'SC_state', 'SD_state', 'TN_state',\n",
    "       'TX_state', 'UNK_state', 'UT_state', 'VA_state', 'WA_state', 'WI_state',\n",
    "       'WV_state', 'WY_state', 'XX_state','rentzestimate_amount']]#change column order\n",
    "property_price_norm.iloc[:,:8]=(property_price_norm.iloc[:,:8]- property_price_norm.iloc[:,:8].mean())/(property_price_norm.iloc[:,:8].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123494, 69)\n",
      "(52926, 69)\n",
      "(153083, 69)\n"
     ]
    }
   ],
   "source": [
    "#property_price_valid -- with rentzestimate_amount value| property_price_tobe -- rentzestimate_amount missing\n",
    "property_price_valid = property_price_norm.loc[property_price_all['rentzestimate_amount'].isnull()==False]\n",
    "property_price_tobe = property_price_norm.loc[property_price_all['rentzestimate_amount'].isnull()==True]\n",
    "\n",
    "#train_set(70% out of property_price_valid), test_set, predict_set\n",
    "train_set = property_price_valid.sample(frac=0.7,random_state=0)\n",
    "test_set = property_price_valid.drop(train_set.index)\n",
    "train_set = train_set.reset_index(drop=True)#reset index of dataframe\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "predict_set = property_price_tobe.reset_index(drop=True)\n",
    "predict_set.iloc[:-1]=predict_set.iloc[:-1].fillna(0)\n",
    "#fill NA in rentzestimate_amount with 0\n",
    "predict_set[['rentzestimate_amount']]= predict_set[['rentzestimate_amount']].fillna(0)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "print(predict_set.shape)\n",
    "\n",
    "#save train_set, test_set, predict_set to csv\n",
    "train_set.to_csv('train_prediction.csv',header = False,index = False)\n",
    "test_set.to_csv('test_prediction.csv',header = False,index = False)\n",
    "predict_set.to_csv('tobe_prediction.csv',header = False,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO) #enable logging--show iteration process\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the structure of neural network with a function: model_fn\n",
    "def model_fn(features, targets, mode, params):\n",
    "    \"\"\"Model function for Estimator.\"\"\"\n",
    "\n",
    "  # Connect the first hidden layer to input layer(features) with relu activation\n",
    "    first_hidden_layer = tf.contrib.layers.fully_connected(features, 30,tf.nn.relu)\n",
    "\n",
    "  # Connect the second hidden layer to first hidden layer with relu\n",
    "    second_hidden_layer = tf.contrib.layers.fully_connected(first_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     third_hidden_layer = tf.contrib.layers.fully_connected(second_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     fourth_hidden_layer = tf.contrib.layers.fully_connected(third_hidden_layer, 30,tf.nn.relu)\n",
    " \n",
    "#     fifth_hidden_layer = tf.contrib.layers.fully_connected(fourth_hidden_layer, 30,tf.nn.relu)\n",
    "  \n",
    "#     sixth_hidden_layer = tf.contrib.layers.fully_connected(fifth_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     seventh_hidden_layer = tf.contrib.layers.fully_connected(sixth_hidden_layer, 30,tf.nn.relu)\n",
    "  \n",
    "#     eighth_hidden_layer = tf.contrib.layers.fully_connected(seventh_hidden_layer, 30,tf.nn.relu)\n",
    "   \n",
    "#     ninth_hidden_layer = tf.contrib.layers.fully_connected(eighth_hidden_layer, 30,tf.nn.relu)\n",
    " \n",
    "#     tenth_hidden_layer = tf.contrib.layers.fully_connected(ninth_hidden_layer, 30,tf.nn.relu)\n",
    "\n",
    "  # Connect the output layer to second hidden layer (no activation fn)\n",
    "    output_layer = tf.contrib.layers.linear(second_hidden_layer, 1)\n",
    "\n",
    "  # Reshape output layer to 1-dim Tensor to return predictions\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    predictions_dict = {\"rent\": predictions}\n",
    "\n",
    "  # Calculate loss using MAPE(mean absolute percentage error)\n",
    "    loss = tf.reduce_mean(tf.abs(1 - (predictions / tf.cast(targets,tf.float32))))\n",
    " \n",
    "  # Calculate mean squared error,root mean squared error, mean absolute error and mape as additional eval metric\n",
    "    eval_metric_ops = {\n",
    "      \"mse\":\n",
    "          tf.metrics.mean_squared_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"rmse\":\n",
    "          tf.metrics.root_mean_squared_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"mae\":\n",
    "          tf.metrics.mean_absolute_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"mape\":\n",
    "          tf.contrib.keras.metrics.mean_absolute_percentage_error(\n",
    "          tf.cast(targets, tf.float32), predictions)}\n",
    "  \n",
    "  # Optimization algorithm to use during training: Adam\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=params[\"learning_rate\"],\n",
    "      optimizer=\"Adam\")\n",
    "\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, #context in which the model_fn was invoked: fit()/evaluate()/predict()\n",
    "      predictions=predictions_dict,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_save_summary_steps': 100, '_num_worker_replicas': 0, '_master': '', '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025C2CD7F1D0>, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_environment': 'local', '_tf_random_seed': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_model_dir': 'C:\\\\Users\\\\HANYUA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfi1_jdo_', '_save_checkpoints_secs': 600, '_task_type': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_session_config': None, '_is_chief': True}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.999977, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.5166\n",
      "INFO:tensorflow:loss = 0.585823, step = 101 (39.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.25304\n",
      "INFO:tensorflow:loss = 0.322676, step = 201 (44.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99668\n",
      "INFO:tensorflow:loss = 0.277567, step = 301 (50.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.03446\n",
      "INFO:tensorflow:loss = 0.255703, step = 401 (49.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12719\n",
      "INFO:tensorflow:loss = 0.232953, step = 501 (47.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.18122\n",
      "INFO:tensorflow:loss = 0.206199, step = 601 (45.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.5319\n",
      "INFO:tensorflow:loss = 0.1826, step = 701 (39.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.39265\n",
      "INFO:tensorflow:loss = 0.173438, step = 801 (41.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.17323\n",
      "INFO:tensorflow:loss = 0.170428, step = 901 (45.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.16394\n",
      "INFO:tensorflow:loss = 0.168277, step = 1001 (46.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.34348\n",
      "INFO:tensorflow:loss = 0.165191, step = 1101 (42.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08924\n",
      "INFO:tensorflow:loss = 0.160244, step = 1201 (47.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.27065\n",
      "INFO:tensorflow:loss = 0.154434, step = 1301 (44.024 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1336 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.52918\n",
      "INFO:tensorflow:loss = 0.150028, step = 1401 (39.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.86721\n",
      "INFO:tensorflow:loss = 0.147279, step = 1501 (34.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.84646\n",
      "INFO:tensorflow:loss = 0.145272, step = 1601 (35.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.90597\n",
      "INFO:tensorflow:loss = 0.144023, step = 1701 (34.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.57596\n",
      "INFO:tensorflow:loss = 0.143182, step = 1801 (38.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.20866\n",
      "INFO:tensorflow:loss = 0.142388, step = 1901 (45.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.97219\n",
      "INFO:tensorflow:loss = 0.141686, step = 2001 (50.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12925\n",
      "INFO:tensorflow:loss = 0.141023, step = 2101 (47.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.27345\n",
      "INFO:tensorflow:loss = 0.140407, step = 2201 (43.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68746\n",
      "INFO:tensorflow:loss = 0.139874, step = 2301 (59.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69803\n",
      "INFO:tensorflow:loss = 0.139378, step = 2401 (58.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.74187\n",
      "INFO:tensorflow:loss = 0.138877, step = 2501 (57.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.74003\n",
      "INFO:tensorflow:loss = 0.138408, step = 2601 (57.468 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2620 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.56294\n",
      "INFO:tensorflow:loss = 0.137922, step = 2701 (63.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71524\n",
      "INFO:tensorflow:loss = 0.137572, step = 2801 (58.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68751\n",
      "INFO:tensorflow:loss = 0.137236, step = 2901 (59.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6404\n",
      "INFO:tensorflow:loss = 0.136927, step = 3001 (60.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.80827\n",
      "INFO:tensorflow:loss = 0.136675, step = 3101 (55.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.87847\n",
      "INFO:tensorflow:loss = 0.136409, step = 3201 (53.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70702\n",
      "INFO:tensorflow:loss = 0.136147, step = 3301 (58.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.73172\n",
      "INFO:tensorflow:loss = 0.135928, step = 3401 (57.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.74276\n",
      "INFO:tensorflow:loss = 0.135721, step = 3501 (57.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.697\n",
      "INFO:tensorflow:loss = 0.135429, step = 3601 (58.860 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3649 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.61003\n",
      "INFO:tensorflow:loss = 0.13523, step = 3701 (62.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.726\n",
      "INFO:tensorflow:loss = 0.13506, step = 3801 (57.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.87424\n",
      "INFO:tensorflow:loss = 0.134863, step = 3901 (53.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06784\n",
      "INFO:tensorflow:loss = 0.134715, step = 4001 (48.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71533\n",
      "INFO:tensorflow:loss = 0.134558, step = 4101 (58.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69402\n",
      "INFO:tensorflow:loss = 0.134448, step = 4201 (59.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.5013\n",
      "INFO:tensorflow:loss = 0.134342, step = 4301 (40.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.83653\n",
      "INFO:tensorflow:loss = 0.134248, step = 4401 (54.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66761\n",
      "INFO:tensorflow:loss = 0.134147, step = 4501 (59.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.30131\n",
      "INFO:tensorflow:loss = 0.134052, step = 4601 (43.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.25363\n",
      "INFO:tensorflow:loss = 0.133933, step = 4701 (23.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.73751\n",
      "INFO:tensorflow:loss = 0.133814, step = 4801 (21.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.68917\n",
      "INFO:tensorflow:loss = 0.133724, step = 4901 (21.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.40716\n",
      "INFO:tensorflow:loss = 0.133586, step = 5001 (22.690 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5011 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.10488\n",
      "INFO:tensorflow:loss = 0.133476, step = 5101 (24.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.06282\n",
      "INFO:tensorflow:loss = 0.13338, step = 5201 (24.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.3623\n",
      "INFO:tensorflow:loss = 0.133297, step = 5301 (22.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.19296\n",
      "INFO:tensorflow:loss = 0.133206, step = 5401 (23.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.5921\n",
      "INFO:tensorflow:loss = 0.133103, step = 5501 (21.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.31763\n",
      "INFO:tensorflow:loss = 0.13304, step = 5601 (30.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.58026\n",
      "INFO:tensorflow:loss = 0.132958, step = 5701 (21.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.54836\n",
      "INFO:tensorflow:loss = 0.132897, step = 5801 (21.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.33393\n",
      "INFO:tensorflow:loss = 0.13282, step = 5901 (23.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.49318\n",
      "INFO:tensorflow:loss = 0.132767, step = 6001 (22.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.13107\n",
      "INFO:tensorflow:loss = 0.132696, step = 6101 (24.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.71891\n",
      "INFO:tensorflow:loss = 0.132655, step = 6201 (21.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.65419\n",
      "INFO:tensorflow:loss = 0.132577, step = 6301 (21.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.34454\n",
      "INFO:tensorflow:loss = 0.132531, step = 6401 (23.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.38158\n",
      "INFO:tensorflow:loss = 0.132455, step = 6501 (22.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.8867\n",
      "INFO:tensorflow:loss = 0.132394, step = 6601 (25.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.54985\n",
      "INFO:tensorflow:loss = 0.132344, step = 6701 (21.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.73623\n",
      "INFO:tensorflow:loss = 0.132263, step = 6801 (21.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.01891\n",
      "INFO:tensorflow:loss = 0.132204, step = 6901 (24.898 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 4.69094\n",
      "INFO:tensorflow:loss = 0.132158, step = 7001 (21.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.29134\n",
      "INFO:tensorflow:loss = 0.13212, step = 7101 (23.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.71224\n",
      "INFO:tensorflow:loss = 0.132067, step = 7201 (21.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.14288\n",
      "INFO:tensorflow:loss = 0.132025, step = 7301 (24.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.67034\n",
      "INFO:tensorflow:loss = 0.131991, step = 7401 (21.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.5738\n",
      "INFO:tensorflow:loss = 0.131938, step = 7501 (21.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.0163\n",
      "INFO:tensorflow:loss = 0.1319, step = 7601 (24.899 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7605 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.9834\n",
      "INFO:tensorflow:loss = 0.131879, step = 7701 (25.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.28187\n",
      "INFO:tensorflow:loss = 0.131854, step = 7801 (23.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.18043\n",
      "INFO:tensorflow:loss = 0.131798, step = 7901 (23.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.4432\n",
      "INFO:tensorflow:loss = 0.13177, step = 8001 (22.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.57503\n",
      "INFO:tensorflow:loss = 0.131729, step = 8101 (21.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.71679\n",
      "INFO:tensorflow:loss = 0.131709, step = 8201 (21.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62649\n",
      "INFO:tensorflow:loss = 0.131663, step = 8301 (21.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.26498\n",
      "INFO:tensorflow:loss = 0.131655, step = 8401 (23.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72628\n",
      "INFO:tensorflow:loss = 0.131609, step = 8501 (57.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.58689\n",
      "INFO:tensorflow:loss = 0.131615, step = 8601 (62.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.73463\n",
      "INFO:tensorflow:loss = 0.131559, step = 8701 (57.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.01284\n",
      "INFO:tensorflow:loss = 0.131532, step = 8801 (49.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.63983\n",
      "INFO:tensorflow:loss = 0.131504, step = 8901 (60.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.5746\n",
      "INFO:tensorflow:loss = 0.131474, step = 9001 (63.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.57904\n",
      "INFO:tensorflow:loss = 0.13146, step = 9101 (63.313 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9105 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.49042\n",
      "INFO:tensorflow:loss = 0.131434, step = 9201 (67.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.57906\n",
      "INFO:tensorflow:loss = 0.131406, step = 9301 (63.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.65855\n",
      "INFO:tensorflow:loss = 0.13137, step = 9401 (60.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67606\n",
      "INFO:tensorflow:loss = 0.131342, step = 9501 (59.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.55015\n",
      "INFO:tensorflow:loss = 0.131311, step = 9601 (64.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.65238\n",
      "INFO:tensorflow:loss = 0.131272, step = 9701 (60.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.54054\n",
      "INFO:tensorflow:loss = 0.131238, step = 9801 (64.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67533\n",
      "INFO:tensorflow:loss = 0.131185, step = 9901 (59.597 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.131144.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-02-05:00:24\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt-10000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-02-05:00:32\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 0.133318, mae = 825.969, mape = 13.3318, mse = 1.79029e+07, rmse = 4231.18\n",
      "Loss: 0.133318\n",
      "Mean Squared Error: 1.79029e+07\n",
      "Root Mean Squared Error: 4231.18\n",
      "Mean Absolute Error: 825.969\n",
      "Mean Absolute Percentage Error: 13.3318\n"
     ]
    }
   ],
   "source": [
    "# Read training_set, testing_set, prediction_set as np.array and define target and features for each dataset\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"train_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "testing_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"test_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "prediction_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"tobe_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "# Set model params\n",
    "model_params = {\"learning_rate\": LEARNING_RATE}\n",
    "\n",
    "# Instantiate estimator by calling tf.contrib.learn.Estimator\n",
    "nn = tf.contrib.learn.Estimator(model_fn=model_fn, params=model_params)\n",
    "\n",
    "# Use input functions(input_fn) to feed feature (x) and label (y) Tensors into the model for training (get_train_inputs())\n",
    "def get_train_inputs():\n",
    "        x = tf.constant(training_set.data)\n",
    "        y = tf.constant(training_set.target)\n",
    "        return x, y\n",
    "\n",
    "# Fit the estimator-nn on training data, no of iterations -- step\n",
    "nn.fit(input_fn=get_train_inputs, steps=10000)\n",
    " \n",
    "# Use input functions(input_fn) to feed feature (x) and label (y) Tensors into the model for evaluation (get_test_inputs())\n",
    "def get_test_inputs():\n",
    "        x = tf.constant(testing_set.data)\n",
    "        y = tf.constant(testing_set.target)\n",
    "        return x, y\n",
    "# Evaluate the estimator-nn on testing data\n",
    "ev = nn.evaluate(input_fn=get_test_inputs, steps=1)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Loss: %s\" % ev[\"loss\"])\n",
    "print(\"Mean Squared Error: %s\" % ev[\"mse\"])\n",
    "print(\"Root Mean Squared Error: %s\" % ev[\"rmse\"])\n",
    "print(\"Mean Absolute Error: %s\" % ev[\"mae\"])\n",
    "print(\"Mean Absolute Percentage Error: %s\" % ev[\"mape\"])\n",
    "    \n",
    "# #print out predictions\n",
    "# predictions = nn.predict(x=prediction_set.data, as_iterable=True)\n",
    "# for i, p in enumerate(predictions):\n",
    "#         print(\"Prediction %s: %s\" % (i + 1, p[\"rent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-f7bbc490d54d>:2: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt-10000\n",
      "WARNING:tensorflow:From <ipython-input-21-f7bbc490d54d>:11: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt-10000\n",
      "WARNING:tensorflow:From <ipython-input-21-f7bbc490d54d>:17: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmpfi1_jdo_\\model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "# Predict on prediction_set\n",
    "predictions_predict = nn.predict(x=prediction_set.data, as_iterable=True)\n",
    "# .predict() returns an iterator; convert to a list\n",
    "import itertools\n",
    "predictions_list_predict = list(itertools.islice(predictions_predict,predict_set.shape[0]))\n",
    "# Combine predictions with prediction_set and save it to csv\n",
    "tf_predict_with_prediction = pd.concat([predict_set,pd.DataFrame(predictions_list_predict)],axis = 1)\n",
    "pd.DataFrame(tf_predict_with_prediction).to_csv('tf_predict_with_prediction_with_state.csv',index = False)\n",
    "\n",
    "# Predictions for training dataset, combine with training set and save it to csv\n",
    "predictions_train = nn.predict(x=training_set.data, as_iterable=True)\n",
    "predictions_list_train = list(itertools.islice(predictions_train,train_set.shape[0]))\n",
    "tf_train_with_prediction = pd.concat([train_set,pd.DataFrame(predictions_list_train)],axis = 1)\n",
    "pd.DataFrame(tf_train_with_prediction).to_csv('tf_train_with_prediction_with_state.csv',index = False)\n",
    "\n",
    "# Predictions for testing dataset, combine with testing set and save it to csv\n",
    "predictions_test = nn.predict(x=testing_set.data, as_iterable=True)\n",
    "predictions_list_test = list(itertools.islice(predictions_test,test_set.shape[0]))\n",
    "tf_test_with_prediction = pd.concat([test_set,pd.DataFrame(predictions_list_test)],axis = 1)\n",
    "pd.DataFrame(tf_test_with_prediction).to_csv('tf_test_with_prediction_with_state.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "   rentzestimate_amount         rent\n",
      "0                1726.0  1857.768921\n",
      "1                1875.0  1928.354980\n",
      "2                1500.0  1462.379639\n",
      "3                2500.0  2239.605469\n",
      "4                2500.0  2431.782227\n",
      "test:\n",
      "   rentzestimate_amount         rent\n",
      "0                3569.0  3469.236572\n",
      "1                1375.0  1101.411987\n",
      "2                1600.0  1638.502197\n",
      "3                1600.0  1441.999634\n",
      "4                1918.0  2122.803955\n",
      "predict:\n",
      "   rentzestimate_amount          rent\n",
      "0                   0.0   2079.402100\n",
      "1                   0.0   1885.790894\n",
      "2                   0.0   2322.862061\n",
      "3                   0.0   1270.009521\n",
      "4                   0.0  10539.256836\n"
     ]
    }
   ],
   "source": [
    "#Take a look of the actual rent and predicted rent for training set, testing set and prediction set\n",
    "print('train:')\n",
    "print(tf_train_with_prediction[['rentzestimate_amount','rent']].head())\n",
    "print('test:')\n",
    "print(tf_test_with_prediction[['rentzestimate_amount','rent']].head())\n",
    "print('predict:')\n",
    "print(tf_predict_with_prediction[['rentzestimate_amount','rent']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
