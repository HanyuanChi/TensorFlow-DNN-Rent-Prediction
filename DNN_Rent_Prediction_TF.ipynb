{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanyuan chi\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# read data\n",
    "rent = pd.read_csv('fang88_rent.csv',header = 0, na_values=[\"NA\",\"NaN\", \" \",\"\",\"NULL\",\"N/A\",\"None\"])\n",
    "house = pd.read_csv('home.csv',header = 0, na_values=[\"NA\",\"NaN\", \" \",\"\", \"NULL\",\"N/A\",\"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       rentzestimate_amount\n",
      "count         960854.000000\n",
      "mean            2994.374168\n",
      "std             4020.805179\n",
      "min              400.000000\n",
      "25%             1550.000000\n",
      "50%             2200.000000\n",
      "75%             3200.000000\n",
      "max           250000.000000\n",
      "rentzestimate_amount    float64\n",
      "unique_id                object\n",
      "dtype: object\n",
      "(1048575, 2)\n"
     ]
    }
   ],
   "source": [
    "#pick the columns to use in rent(unique_id and rentzestimate_amount) and check the statistics of rent dataset\n",
    "rent_use = rent.iloc[:,4:6] #rent_use = rent[[\"rentzestimate_amount\",\"unique_id\"]]\n",
    "print(rent_use.describe())\n",
    "print(rent_use.dtypes)\n",
    "print(rent_use.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rentzestimate_amount    unique_id\n",
      "1902                  1800.0  5.74132E+13\n",
      "8565                     NaN  5.81157E+13\n",
      "9169                     NaN  5.82614E+13\n",
      "422191                2962.0  5.89261E+13\n"
     ]
    }
   ],
   "source": [
    "#remove duplicated unique_id in rent\n",
    "print(rent_use[rent_use.duplicated(subset = 'unique_id',keep = 'first') == True])\n",
    "rent_use = rent_use.drop_duplicates(subset = 'unique_id',keep = 'first') #4 duplicates removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           bathrooms       bedrooms    list_price       latitude  \\\n",
      "count  349570.000000  338944.000000  3.522200e+05  338114.000000   \n",
      "mean        2.478044       3.364178  6.903779e+05      33.730169   \n",
      "std         1.339578       1.313034  1.763565e+06       5.998713   \n",
      "min         0.000000       0.000000  0.000000e+00     -81.265730   \n",
      "25%         2.000000       3.000000  2.199990e+05      29.495881   \n",
      "50%         2.000000       3.000000  3.699000e+05      33.208160   \n",
      "75%         3.000000       4.000000  6.499000e+05      37.939740   \n",
      "max        99.990000      99.000000  2.500000e+08      49.002033   \n",
      "\n",
      "           longitude      lot_sqft          sqft    year_built  \n",
      "count  338114.000000  2.141130e+05  3.268170e+05  3.360800e+05  \n",
      "mean      -93.934758  1.301220e+05  2.368388e+03  2.047163e+03  \n",
      "std        16.612686  8.698167e+06  6.622419e+03  3.375856e+04  \n",
      "min      -159.676034  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%       -97.995093  5.600000e+03  1.344000e+03  1.970000e+03  \n",
      "50%       -88.116701  8.712000e+03  2.012000e+03  1.993000e+03  \n",
      "75%       -81.483450  1.864400e+04  2.961000e+03  2.006000e+03  \n",
      "max         1.923200  2.147484e+09  3.092760e+06  1.957194e+07  \n",
      "unique_id          object\n",
      " bathrooms        float64\n",
      " bedrooms         float64\n",
      " city              object\n",
      " list_price       float64\n",
      " latitude         float64\n",
      " longitude        float64\n",
      " property_type     object\n",
      " lot_sqft         float64\n",
      " sqft             float64\n",
      " state             object\n",
      " year_built       float64\n",
      " zip               object\n",
      "dtype: object\n",
      "Empty DataFrame\n",
      "Columns: [unique_id,  bathrooms,  bedrooms,  city,  list_price,  latitude,  longitude,  property_type,  lot_sqft,  sqft,  state,  year_built,  zip]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#check the statistics of house dataset and remove duplicated unique_id in house\n",
    "print(house.describe())\n",
    "print(house.dtypes)\n",
    "print(house[house.duplicated(subset = 'unique_id',keep = 'first') == True])#no duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           bathrooms       bedrooms    list_price       latitude  \\\n",
      "count  349570.000000  338944.000000  3.522200e+05  338114.000000   \n",
      "mean        2.478044       3.364178  6.903779e+05      33.730169   \n",
      "std         1.339578       1.313034  1.763565e+06       5.998713   \n",
      "min         0.000000       0.000000  0.000000e+00     -81.265730   \n",
      "25%         2.000000       3.000000  2.199990e+05      29.495881   \n",
      "50%         2.000000       3.000000  3.699000e+05      33.208160   \n",
      "75%         3.000000       4.000000  6.499000e+05      37.939740   \n",
      "max        99.990000      99.000000  2.500000e+08      49.002033   \n",
      "\n",
      "           longitude      lot_sqft          sqft    year_built  \\\n",
      "count  338114.000000  2.141130e+05  3.268170e+05  3.360800e+05   \n",
      "mean      -93.934758  1.301220e+05  2.368388e+03  2.047163e+03   \n",
      "std        16.612686  8.698167e+06  6.622419e+03  3.375856e+04   \n",
      "min      -159.676034  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%       -97.995093  5.600000e+03  1.344000e+03  1.970000e+03   \n",
      "50%       -88.116701  8.712000e+03  2.012000e+03  1.993000e+03   \n",
      "75%       -81.483450  1.864400e+04  2.961000e+03  2.006000e+03   \n",
      "max         1.923200  2.147484e+09  3.092760e+06  1.957194e+07   \n",
      "\n",
      "       rentzestimate_amount  \n",
      "count         187277.000000  \n",
      "mean            3854.405378  \n",
      "std             6320.721666  \n",
      "min              400.000000  \n",
      "25%             1600.000000  \n",
      "50%             2300.000000  \n",
      "75%             3500.000000  \n",
      "max           250000.000000  \n",
      "unique_id                object\n",
      " bathrooms              float64\n",
      " bedrooms               float64\n",
      " city                    object\n",
      " list_price             float64\n",
      " latitude               float64\n",
      " longitude              float64\n",
      " property_type           object\n",
      " lot_sqft               float64\n",
      " sqft                   float64\n",
      " state                   object\n",
      " year_built             float64\n",
      " zip                     object\n",
      "rentzestimate_amount    float64\n",
      "dtype: object\n",
      "(352221, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>city</th>\n",
       "      <th>list_price</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>property_type</th>\n",
       "      <th>lot_sqft</th>\n",
       "      <th>sqft</th>\n",
       "      <th>state</th>\n",
       "      <th>year_built</th>\n",
       "      <th>zip</th>\n",
       "      <th>rentzestimate_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABOR_15646314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>30.388230</td>\n",
       "      <td>-97.965567</td>\n",
       "      <td>RESI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>78734</td>\n",
       "      <td>1350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABOR_16633908</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Burnet</td>\n",
       "      <td>6850000.0</td>\n",
       "      <td>30.674729</td>\n",
       "      <td>-98.212187</td>\n",
       "      <td>RESI</td>\n",
       "      <td>640.0</td>\n",
       "      <td>4889.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>78611</td>\n",
       "      <td>9320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABOR_17615028</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>San Marcos</td>\n",
       "      <td>334900.0</td>\n",
       "      <td>29.866884</td>\n",
       "      <td>-97.968936</td>\n",
       "      <td>COND</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>78666</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABOR_18247858</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>735000.0</td>\n",
       "      <td>30.265435</td>\n",
       "      <td>-97.737834</td>\n",
       "      <td>APT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>78701</td>\n",
       "      <td>3569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABOR_18359329</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Austin</td>\n",
       "      <td>2800000.0</td>\n",
       "      <td>30.325342</td>\n",
       "      <td>-97.779271</td>\n",
       "      <td>RESI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>78746</td>\n",
       "      <td>13766.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id   bathrooms   bedrooms        city   list_price   latitude  \\\n",
       "0  ABOR_15646314         1.0        1.0      Austin     225000.0  30.388230   \n",
       "1  ABOR_16633908         5.0        3.0      Burnet    6850000.0  30.674729   \n",
       "2  ABOR_17615028         3.0        3.0  San Marcos     334900.0  29.866884   \n",
       "3  ABOR_18247858         2.0        2.0      Austin     735000.0  30.265435   \n",
       "4  ABOR_18359329         3.0        4.0      Austin    2800000.0  30.325342   \n",
       "\n",
       "    longitude  property_type   lot_sqft    sqft  state   year_built    zip  \\\n",
       "0  -97.965567           RESI        0.0   704.0     TX       1955.0  78734   \n",
       "1  -98.212187           RESI      640.0  4889.0     TX       2000.0  78611   \n",
       "2  -97.968936           COND        0.0  2351.0     TX       2016.0  78666   \n",
       "3  -97.737834            APT        0.0  1416.0     TX       2004.0  78701   \n",
       "4  -97.779271           RESI        1.0  2016.0     TX       1962.0  78746   \n",
       "\n",
       "   rentzestimate_amount  \n",
       "0                1350.0  \n",
       "1                9320.0  \n",
       "2                   NaN  \n",
       "3                3569.0  \n",
       "4               13766.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#left join house with rent_use by unique_id\n",
    "house_rent = house.merge(rent_use,left_on = 'unique_id',right_on = 'unique_id',how = 'left')\n",
    "print(house_rent.describe())\n",
    "print(house_rent.dtypes)\n",
    "print(house_rent.shape)\n",
    "house_rent.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244862\n"
     ]
    }
   ],
   "source": [
    "print(len(house_rent[house_rent.isnull().any(axis=1)]))#check the number of NAs\n",
    "#fill NA with median for numerical columns expect for zip and rentzestimate_amount\n",
    "house_rent.iloc[:,:12] = house_rent.iloc[:,:12].fillna(house_rent.iloc[:,:12].median()) \n",
    "# #rename the 14th column as rentzestimate_amount\n",
    "# house_rent.columns.values[13]= 'rentzestimate_amount'\n",
    "house_rent.columns = house_rent.columns.str.strip() #strip white space from the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 19.0 99.99\n",
      "0.0 1.0 20.0 99.0\n",
      "0.0 1450.0 55000000.0 250000000.0\n",
      "0.0 0.0 6992861.04 2147483647.0\n",
      "0.0 0.0 20687.21 3092760.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hanyuan chi\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#Deal with outliers and abnormal values in each column\n",
    "#bathrooms: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.bathrooms,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.bathrooms > q99_99,'bathrooms'] = q99_99\n",
    "np.percentile(house_rent.bathrooms,[0,1,99.99,100])\n",
    "\n",
    "#bedrooms: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.bedrooms,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.bedrooms > q99_99,'bedrooms'] = q99_99\n",
    "np.percentile(house_rent.bedrooms,[0,1,99.99,100])\n",
    "\n",
    "#list_price: capping of outliers--0.9999\n",
    "q0,q1,q99_99,q100 = np.percentile(house_rent.list_price,[0,1,99.99,100])\n",
    "print(q0,q1,q99_99,q100)\n",
    "house_rent.loc[house_rent.list_price > q99_99,'list_price'] = q99_99\n",
    "np.percentile(house_rent.list_price,[0,1,99.99,100])\n",
    "\n",
    "#lot_sqft: capping of outliers--0.9995\n",
    "q0,q1,q99_95,q100 = np.percentile(house_rent.lot_sqft,[0,1,99.95,100])\n",
    "print(q0,q1,q99_95,q100)\n",
    "house_rent.loc[house_rent.lot_sqft > q99_95,'lot_sqft'] = q99_95\n",
    "np.percentile(house_rent.lot_sqft,[0,1,99.95,100])\n",
    "\n",
    "#sqft: capping of outliers--0.9995\n",
    "q0,q1,q99_95,q100 = np.percentile(house_rent.sqft,[0,1,99.95,100])\n",
    "print(q0,q1,q99_95,q100)\n",
    "house_rent.loc[house_rent.sqft > q99_95,'sqft'] = q99_95\n",
    "np.percentile(house_rent.sqft,[0,1,99.95,100])\n",
    "\n",
    "#for year_built, earlier than 1000 and later than 2017 will be replaced by median\n",
    "house_rent.loc[(house_rent.year_built < float(1000)) | \n",
    "               (house_rent.year_built > float(2017)),'year_built'] = house_rent.year_built.median()\n",
    "\n",
    "#replace hi with HI in state, Hi with HI, Ha with HA and Unk with UNK\n",
    "house_rent.loc[house_rent.state == 'hi','state'] = \"HI\"\n",
    "house_rent.loc[house_rent.state == 'Hi','state'] = \"HI\"\n",
    "house_rent.loc[house_rent.state == 'Ha','state'] = \"HA\"\n",
    "house_rent.loc[house_rent.state == 'Unk','state'] = \"UNK\"\n",
    "\n",
    "#select main states\n",
    "main_state = [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\n",
    "                                              \"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\n",
    "                                              \"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\n",
    "                                              \"TN\",\"TX\",\"UT\",\"VA\", \"VT\",\"WA\", \"WI\", \"WV\", \"WY\",\n",
    "                                              \"XX\",\"HI\",\"UNK\",\"HA\",\"BJ\"]\n",
    "house_rent = house_rent.loc[house_rent['state'].isin(main_state)]\n",
    "\n",
    "#choose rows with zip not being NA and has exactly 5 digits\n",
    "house_rent = house_rent.loc[house_rent['zip'].isnull()== False]\n",
    "house_rent.zip = house_rent.zip.astype(str).str.extract('(\\d{5})').astype(float)\n",
    "house_rent = house_rent.loc[house_rent['zip'].isnull()== False]\n",
    "#np.sort(house_rent.zip.unique())\n",
    "\n",
    "#chose rows with property_type not being NA and replace Other with OTHER\n",
    "house_rent = house_rent.loc[house_rent['property_type'].isnull()== False]\n",
    "house_rent.loc[house_rent.property_type == 'Other','property_type'] = 'OTHER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329503, 14)\n",
      "unique_id               329503\n",
      "bathrooms               329503\n",
      "bedrooms                329503\n",
      "city                    329238\n",
      "list_price              329503\n",
      "latitude                329503\n",
      "longitude               329503\n",
      "property_type           329503\n",
      "lot_sqft                329503\n",
      "sqft                    329503\n",
      "state                   329503\n",
      "year_built              329503\n",
      "zip                     329503\n",
      "rentzestimate_amount    176420\n",
      "dtype: int64\n",
      "unique_id                object\n",
      "bathrooms               float64\n",
      "bedrooms                float64\n",
      "city                     object\n",
      "list_price              float64\n",
      "latitude                float64\n",
      "longitude               float64\n",
      "property_type            object\n",
      "lot_sqft                float64\n",
      "sqft                    float64\n",
      "state                    object\n",
      "year_built              float64\n",
      "zip                     float64\n",
      "rentzestimate_amount    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check data structure\n",
    "print(house_rent.shape)\n",
    "print(house_rent.count())\n",
    "print(house_rent.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change zip code data type from float to string\n",
    "house_rent.zip = house_rent.zip.astype(str)\n",
    "#create new variable: year_from_now\n",
    "import datetime\n",
    "house_rent['year_from_now'] = datetime.date.today().year - house_rent['year_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#categorical variables: create dummies--property type\n",
    "#zip,city,state have too many levels, so I don't include them in the prediction\n",
    "# state = pd.get_dummies(house_rent['state']).add_suffix('_state')\n",
    "property_type = pd.get_dummies(house_rent['property_type']).add_suffix('_type')\n",
    "house_rent_dummy = pd.concat([house_rent,property_type],axis = 1)#combine dummy and the original dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select useful attributes\n",
    "property_price_all = house_rent_dummy.loc[:,~house_rent_dummy.columns.isin(['Unnamed: 0','unique_id','city','property_type',\n",
    "                                                                        'state','year_built','zip'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalization: (x-mean)/std (dummy variable and y(rentzestimate_amount) don't need normalization)\n",
    "property_price_norm = property_price_all\n",
    "property_price_norm = property_price_norm[['bathrooms', 'bedrooms', 'list_price', 'latitude', 'longitude',\n",
    "       'lot_sqft', 'sqft', 'year_from_now', 'APT_type',\n",
    "       'COND_type', 'COOP_type', 'Country Homes/Acreag_type', 'FARM_type',\n",
    "       'LAND_type', 'MULT_type', 'OTHER_type', 'RENT_type', 'RESI_type',\n",
    "       'Timeshare_type','rentzestimate_amount']]#change column order\n",
    "property_price_norm.iloc[:,:8]=(property_price_norm.iloc[:,:8]- property_price_norm.iloc[:,:8].mean())/(property_price_norm.iloc[:,:8].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123494, 20)\n",
      "(52926, 20)\n",
      "(153083, 20)\n"
     ]
    }
   ],
   "source": [
    "#property_price_valid -- with rentzestimate_amount value| property_price_tobe -- rentzestimate_amount missing\n",
    "property_price_valid = property_price_norm.loc[property_price_all['rentzestimate_amount'].isnull()==False]\n",
    "property_price_tobe = property_price_norm.loc[property_price_all['rentzestimate_amount'].isnull()==True]\n",
    "\n",
    "#train_set(70% out of property_price_valid), test_set, predict_set\n",
    "train_set = property_price_valid.sample(frac=0.7,random_state=0)\n",
    "test_set = property_price_valid.drop(train_set.index)\n",
    "train_set = train_set.reset_index(drop=True)#reset index of dataframe\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "predict_set = property_price_tobe.reset_index(drop=True)\n",
    "predict_set.iloc[:-1]=predict_set.iloc[:-1].fillna(0)\n",
    "#fill NA in rentzestimate_amount with 0\n",
    "predict_set[['rentzestimate_amount']]= predict_set[['rentzestimate_amount']].fillna(0)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "print(predict_set.shape)\n",
    "\n",
    "#save train_set, test_set, predict_set to csv\n",
    "train_set.to_csv('train_prediction.csv',header = False,index = False)\n",
    "test_set.to_csv('test_prediction.csv',header = False,index = False)\n",
    "predict_set.to_csv('tobe_prediction.csv',header = False,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO) #enable logging--show iteration process\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the structure of neural network with a function: model_fn\n",
    "def model_fn(features, targets, mode, params):\n",
    "    \"\"\"Model function for Estimator.\"\"\"\n",
    "\n",
    "  # Connect the first hidden layer to input layer(features) with relu activation\n",
    "    first_hidden_layer = tf.contrib.layers.fully_connected(features, 30,tf.nn.relu)\n",
    "\n",
    "  # Connect the second hidden layer to first hidden layer with relu\n",
    "    second_hidden_layer = tf.contrib.layers.fully_connected(first_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     third_hidden_layer = tf.contrib.layers.fully_connected(second_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     fourth_hidden_layer = tf.contrib.layers.fully_connected(third_hidden_layer, 30,tf.nn.relu)\n",
    " \n",
    "#     fifth_hidden_layer = tf.contrib.layers.fully_connected(fourth_hidden_layer, 30,tf.nn.relu)\n",
    "  \n",
    "#     sixth_hidden_layer = tf.contrib.layers.fully_connected(fifth_hidden_layer, 30,tf.nn.relu)\n",
    "    \n",
    "#     seventh_hidden_layer = tf.contrib.layers.fully_connected(sixth_hidden_layer, 30,tf.nn.relu)\n",
    "  \n",
    "#     eighth_hidden_layer = tf.contrib.layers.fully_connected(seventh_hidden_layer, 30,tf.nn.relu)\n",
    "   \n",
    "#     ninth_hidden_layer = tf.contrib.layers.fully_connected(eighth_hidden_layer, 30,tf.nn.relu)\n",
    " \n",
    "#     tenth_hidden_layer = tf.contrib.layers.fully_connected(ninth_hidden_layer, 30,tf.nn.relu)\n",
    "\n",
    "  # Connect the output layer to second hidden layer (no activation fn)\n",
    "    output_layer = tf.contrib.layers.linear(second_hidden_layer, 1)\n",
    "\n",
    "  # Reshape output layer to 1-dim Tensor to return predictions\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    predictions_dict = {\"rent\": predictions}\n",
    "\n",
    "  # Calculate loss using MAPE(mean absolute percentage error)\n",
    "    loss = tf.reduce_mean(tf.abs(1 - (predictions / tf.cast(targets,tf.float32))))\n",
    " \n",
    "  # Calculate mean squared error,root mean squared error, mean absolute error and mape as additional eval metric\n",
    "    eval_metric_ops = {\n",
    "      \"mse\":\n",
    "          tf.metrics.mean_squared_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"rmse\":\n",
    "          tf.metrics.root_mean_squared_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"mae\":\n",
    "          tf.metrics.mean_absolute_error(\n",
    "              tf.cast(targets, tf.float32), predictions),\n",
    "      \"mape\":\n",
    "          tf.contrib.keras.metrics.mean_absolute_percentage_error(\n",
    "          tf.cast(targets, tf.float32), predictions)}\n",
    "  \n",
    "  # Optimization algorithm to use during training: Adam\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=params[\"learning_rate\"],\n",
    "      optimizer=\"Adam\")\n",
    "\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, #context in which the model_fn was invoked: fit()/evaluate()/predict()\n",
    "      predictions=predictions_dict,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_master': '', '_tf_random_seed': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023396D207B8>, '_num_worker_replicas': 0, '_num_ps_replicas': 0, '_environment': 'local', '_model_dir': 'C:\\\\Users\\\\HANYUA~1\\\\AppData\\\\Local\\\\Temp\\\\tmp22kbln2c', '_session_config': None, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.79811\n",
      "INFO:tensorflow:loss = 0.37454, step = 101 (20.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20957\n",
      "INFO:tensorflow:loss = 0.292042, step = 201 (23.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.61165\n",
      "INFO:tensorflow:loss = 0.239486, step = 301 (21.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.71199\n",
      "INFO:tensorflow:loss = 0.187018, step = 401 (26.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.40878\n",
      "INFO:tensorflow:loss = 0.174095, step = 501 (22.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.50388\n",
      "INFO:tensorflow:loss = 0.1704, step = 601 (22.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.69108\n",
      "INFO:tensorflow:loss = 0.166968, step = 701 (21.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.21577\n",
      "INFO:tensorflow:loss = 0.161569, step = 801 (19.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.66302\n",
      "INFO:tensorflow:loss = 0.157391, step = 901 (21.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.46706\n",
      "INFO:tensorflow:loss = 0.152998, step = 1001 (22.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.50793\n",
      "INFO:tensorflow:loss = 0.14895, step = 1101 (22.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.34577\n",
      "INFO:tensorflow:loss = 0.146178, step = 1201 (23.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.36978\n",
      "INFO:tensorflow:loss = 0.143954, step = 1301 (22.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.85633\n",
      "INFO:tensorflow:loss = 0.14244, step = 1401 (20.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.8418\n",
      "INFO:tensorflow:loss = 0.141242, step = 1501 (20.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.0066\n",
      "INFO:tensorflow:loss = 0.140402, step = 1601 (16.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93287\n",
      "INFO:tensorflow:loss = 0.139864, step = 1701 (16.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.31693\n",
      "INFO:tensorflow:loss = 0.139431, step = 1801 (18.800 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35531\n",
      "INFO:tensorflow:loss = 0.139094, step = 1901 (15.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.78027\n",
      "INFO:tensorflow:loss = 0.138847, step = 2001 (14.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.40536\n",
      "INFO:tensorflow:loss = 0.138592, step = 2101 (15.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.8404\n",
      "INFO:tensorflow:loss = 0.138382, step = 2201 (17.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.74765\n",
      "INFO:tensorflow:loss = 0.138211, step = 2301 (21.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.10208\n",
      "INFO:tensorflow:loss = 0.138068, step = 2401 (24.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.61382\n",
      "INFO:tensorflow:loss = 0.137882, step = 2501 (21.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97749\n",
      "INFO:tensorflow:loss = 0.137724, step = 2601 (20.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62278\n",
      "INFO:tensorflow:loss = 0.13756, step = 2701 (21.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.68051\n",
      "INFO:tensorflow:loss = 0.137437, step = 2801 (21.368 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2896 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.11591\n",
      "INFO:tensorflow:loss = 0.137348, step = 2901 (24.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.48797\n",
      "INFO:tensorflow:loss = 0.137184, step = 3001 (22.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62809\n",
      "INFO:tensorflow:loss = 0.137014, step = 3101 (21.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.75237\n",
      "INFO:tensorflow:loss = 0.136923, step = 3201 (21.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.77774\n",
      "INFO:tensorflow:loss = 0.136769, step = 3301 (20.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.30462\n",
      "INFO:tensorflow:loss = 0.136553, step = 3401 (23.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.3077\n",
      "INFO:tensorflow:loss = 0.136368, step = 3501 (23.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.6147\n",
      "INFO:tensorflow:loss = 0.136232, step = 3601 (21.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.91256\n",
      "INFO:tensorflow:loss = 0.136114, step = 3701 (20.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.67165\n",
      "INFO:tensorflow:loss = 0.136032, step = 3801 (21.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.01715\n",
      "INFO:tensorflow:loss = 0.135935, step = 3901 (24.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.08744\n",
      "INFO:tensorflow:loss = 0.13579, step = 4001 (24.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.2396\n",
      "INFO:tensorflow:loss = 0.135662, step = 4101 (23.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.442\n",
      "INFO:tensorflow:loss = 0.135602, step = 4201 (22.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.23293\n",
      "INFO:tensorflow:loss = 0.135455, step = 4301 (19.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.25366\n",
      "INFO:tensorflow:loss = 0.135365, step = 4401 (19.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.16336\n",
      "INFO:tensorflow:loss = 0.135251, step = 4501 (19.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97304\n",
      "INFO:tensorflow:loss = 0.135205, step = 4601 (20.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.61695\n",
      "INFO:tensorflow:loss = 0.135105, step = 4701 (17.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.76946\n",
      "INFO:tensorflow:loss = 0.13504, step = 4801 (17.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.12525\n",
      "INFO:tensorflow:loss = 0.134979, step = 4901 (19.523 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.134981.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-02-05:27:05\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-02-05:27:06\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 0.136689, mae = 837.059, mape = 13.6689, mse = 1.68823e+07, rmse = 4108.81\n",
      "Loss: 0.136689\n",
      "Mean Squared Error: 1.68823e+07\n",
      "Root Mean Squared Error: 4108.81\n",
      "Mean Absolute Error: 837.059\n",
      "Mean Absolute Percentage Error: 13.6689\n"
     ]
    }
   ],
   "source": [
    "# Read training_set, testing_set, prediction_set as np.array and define target and features for each dataset\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"train_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "testing_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"test_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "prediction_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=\"tobe_prediction.csv\", target_dtype=np.float32, features_dtype=np.float32)\n",
    "\n",
    "# Set model params\n",
    "model_params = {\"learning_rate\": LEARNING_RATE}\n",
    "\n",
    "# Instantiate estimator by calling tf.contrib.learn.Estimator\n",
    "nn = tf.contrib.learn.Estimator(model_fn=model_fn, params=model_params)\n",
    "\n",
    "# Use input functions(input_fn) to feed feature (x) and label (y) Tensors into the model for training (get_train_inputs())\n",
    "def get_train_inputs():\n",
    "        x = tf.constant(training_set.data)\n",
    "        y = tf.constant(training_set.target)\n",
    "        return x, y\n",
    "\n",
    "# Fit the estimator-nn on training data, no of iterations -- step\n",
    "nn.fit(input_fn=get_train_inputs, steps=5000)\n",
    " \n",
    "# Use input functions(input_fn) to feed feature (x) and label (y) Tensors into the model for evaluation (get_test_inputs())\n",
    "def get_test_inputs():\n",
    "        x = tf.constant(testing_set.data)\n",
    "        y = tf.constant(testing_set.target)\n",
    "        return x, y\n",
    "# Evaluate the estimator-nn on testing data\n",
    "ev = nn.evaluate(input_fn=get_test_inputs, steps=1)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Loss: %s\" % ev[\"loss\"])\n",
    "print(\"Mean Squared Error: %s\" % ev[\"mse\"])\n",
    "print(\"Root Mean Squared Error: %s\" % ev[\"rmse\"])\n",
    "print(\"Mean Absolute Error: %s\" % ev[\"mae\"])\n",
    "print(\"Mean Absolute Percentage Error: %s\" % ev[\"mape\"])\n",
    "    \n",
    "# #print out predictions\n",
    "# predictions = nn.predict(x=prediction_set.data, as_iterable=True)\n",
    "# for i, p in enumerate(predictions):\n",
    "#         print(\"Prediction %s: %s\" % (i + 1, p[\"rent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-b6843df4a3e4>:2: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt-5000\n",
      "WARNING:tensorflow:From <ipython-input-21-b6843df4a3e4>:11: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt-5000\n",
      "WARNING:tensorflow:From <ipython-input-21-b6843df4a3e4>:17: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HANYUA~1\\AppData\\Local\\Temp\\tmp22kbln2c\\model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "# Predict on prediction_set\n",
    "predictions_predict = nn.predict(x=prediction_set.data, as_iterable=True)\n",
    "# .predict() returns an iterator; convert to a list\n",
    "import itertools\n",
    "predictions_list_predict = list(itertools.islice(predictions_predict,predict_set.shape[0]))\n",
    "# Combine predictions with prediction_set and save it to csv\n",
    "tf_predict_with_prediction = pd.concat([predict_set,pd.DataFrame(predictions_list_predict)],axis = 1)\n",
    "pd.DataFrame(tf_predict_with_prediction).to_csv('tf_predict_with_prediction.csv',index = False)\n",
    "\n",
    "# Predictions for training dataset, combine with training set and save it to csv\n",
    "predictions_train = nn.predict(x=training_set.data, as_iterable=True)\n",
    "predictions_list_train = list(itertools.islice(predictions_train,train_set.shape[0]))\n",
    "tf_train_with_prediction = pd.concat([train_set,pd.DataFrame(predictions_list_train)],axis = 1)\n",
    "pd.DataFrame(tf_train_with_prediction).to_csv('tf_train_with_prediction.csv',index = False)\n",
    "\n",
    "# Predictions for testing dataset, combine with testing set and save it to csv\n",
    "predictions_test = nn.predict(x=testing_set.data, as_iterable=True)\n",
    "predictions_list_test = list(itertools.islice(predictions_test,test_set.shape[0]))\n",
    "tf_test_with_prediction = pd.concat([test_set,pd.DataFrame(predictions_list_test)],axis = 1)\n",
    "pd.DataFrame(tf_test_with_prediction).to_csv('tf_test_with_prediction.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "   rentzestimate_amount         rent\n",
      "0                1726.0  1865.289551\n",
      "1                1875.0  1942.893555\n",
      "2                1500.0  1439.748657\n",
      "3                2500.0  2099.975586\n",
      "4                2500.0  2397.148438\n",
      "test:\n",
      "   rentzestimate_amount         rent\n",
      "0                3569.0  3716.489746\n",
      "1                1375.0  1170.686279\n",
      "2                1600.0  1699.023926\n",
      "3                1600.0  1542.784058\n",
      "4                1918.0  2228.765381\n",
      "predict:\n",
      "   rentzestimate_amount          rent\n",
      "0                   0.0   2338.677734\n",
      "1                   0.0   2093.541992\n",
      "2                   0.0   2495.868896\n",
      "3                   0.0    688.415833\n",
      "4                   0.0  16361.522461\n"
     ]
    }
   ],
   "source": [
    "#Take a look of the actual rent and predicted rent for training set, testing set and prediction set\n",
    "print('train:')\n",
    "print(tf_train_with_prediction[['rentzestimate_amount','rent']].head())\n",
    "print('test:')\n",
    "print(tf_test_with_prediction[['rentzestimate_amount','rent']].head())\n",
    "print('predict:')\n",
    "print(tf_predict_with_prediction[['rentzestimate_amount','rent']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
